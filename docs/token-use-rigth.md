# Token 使用优化指南

## 📊 理解 Token 交互数量

### 什么是 Token？

**Token** 是文本处理的基本单位，在 LLM（大语言模型）中：
- 可能是单词、子词、字符或标点符号
- 具体取决于模型使用的分词方法（Tokenizer）
- 例如：英文单词通常 1-2 个 token，中文汉字通常 1-2 个 token

### Token 交互的组成部分

每次与大模型交互时，token 消耗包括两部分：

#### 1. 输入 Token（Input Tokens）
- **你的提示词**：你发送给 AI 的文本
- **上下文内容**：当前打开的文件、对话历史等
- **系统提示**：Cursor 自动添加的上下文信息

#### 2. 输出 Token（Output Tokens）
- **AI 的回复**：模型生成的文本内容
- **代码补全**：Tab 补全生成的代码片段

#### 总 Token 数 = 输入 Token + 输出 Token

### Token 数量计算示例

#### 示例 1：简单对话

**输入（你的问题）：**
```
"帮我写一个 Java 方法计算两个数的和"
```
- Token 数：约 15-20 tokens（中文 + 英文）

**输出（AI 回复）：**
```java
public int add(int a, int b) {
    return a + b;
}
```
- Token 数：约 20-25 tokens（代码 + 注释）

**本次交互总 Token：约 35-45 tokens**

#### 示例 2：代码审查（高消耗场景）

**输入：**
- 你的提示词：50 tokens
- 打开的文件内容：2000 tokens（500 行代码）
- 对话历史：500 tokens
- **输入总计：约 2550 tokens**

**输出：**
- AI 的详细审查报告：800 tokens

**本次交互总 Token：约 3350 tokens**

#### 示例 3：多文件编辑（Composer）

**输入：**
- 你的需求描述：100 tokens
- 3 个打开的文件：6000 tokens
- 项目上下文：2000 tokens
- **输入总计：约 8100 tokens**

**输出：**
- 修改后的代码：3000 tokens

**本次交互总 Token：约 11100 tokens**

### Cursor 中的 Token 使用场景

#### 1. Chat 模式（单文件问答）
- **输入 Token**：你的问题 + 当前文件内容 + 对话历史
- **输出 Token**：AI 的回复
- **典型消耗**：500-5000 tokens/次

#### 2. Composer 模式（多文件编辑）
- **输入 Token**：需求描述 + 多个文件内容 + 项目上下文
- **输出 Token**：生成的代码修改
- **典型消耗**：5000-50000 tokens/次

#### 3. Tab 补全（代码自动补全）
- **输入 Token**：当前代码上下文（通常较小）
- **输出 Token**：补全的代码片段
- **典型消耗**：50-500 tokens/次

#### 4. 代码生成（Cmd/Ctrl + K）
- **输入 Token**：你的提示 + 当前文件内容
- **输出 Token**：生成的代码
- **典型消耗**：500-3000 tokens/次

### Token 数量估算方法

#### 粗略估算规则

**英文文本：**
- 1 个单词 ≈ 1-1.5 tokens
- 100 个单词 ≈ 130 tokens

**中文文本：**
- 1 个汉字 ≈ 1-2 tokens
- 100 个汉字 ≈ 150 tokens

**代码：**
- 1 行代码 ≈ 5-15 tokens（取决于复杂度）
- 100 行代码 ≈ 1000 tokens

#### 实际计算工具

**在线工具：**
- OpenAI Tokenizer：https://platform.openai.com/tokenizer
- Hugging Face Tokenizer：https://huggingface.co/spaces/Xenova/the-tokenizer

**在 Cursor 中查看：**
- 某些情况下，Cursor 会在响应中显示 token 使用情况
- 可以在设置中查看使用统计

### 影响 Token 数量的因素

#### 1. 上下文长度
- **打开的文件数量**：每个文件都会增加输入 token
- **文件大小**：大文件消耗更多 token
- **对话历史长度**：历史对话会累积

#### 2. 提示词复杂度
- **简单问题**：10-50 tokens
- **复杂需求**：100-500 tokens
- **详细说明**：500+ tokens

#### 3. 输出长度
- **简短回复**：50-200 tokens
- **详细解释**：500-2000 tokens
- **完整代码生成**：1000-10000 tokens

#### 4. 模型选择
- **不同模型**：token 计算方式可能不同
- **某些模型**：可能更高效（更少的 token 表示相同内容）

### Token 成本计算（以 Cursor 为例）

根据你的使用记录：
- **平均每次交互**：约 50-55 万 tokens
- **成本**：约 $0.16-$0.18/次

**换算：**
- 1 万 tokens ≈ $0.003-$0.0033
- 1000 tokens ≈ $0.0003-$0.00033

**这意味着：**
- 一次简单的 Chat（1000 tokens）≈ $0.0003
- 一次代码审查（10000 tokens）≈ $0.003
- 一次多文件编辑（100000 tokens）≈ $0.03

### 监控 Token 使用

#### 在 Cursor 中查看
1. **使用统计页面**：查看总体使用情况
2. **单次交互**：某些响应会显示 token 数
3. **月度报告**：查看详细的使用记录

#### 优化建议
- **定期检查**：每周查看使用统计
- **识别高消耗操作**：找出消耗 token 最多的操作
- **优化策略**：根据实际情况调整使用方式

---

## 减少 Token 使用量的实用方法

### 1. 优化对话方式

#### 使用代码引用而非完整代码
```markdown
❌ 不好：直接粘贴 200 行代码
✅ 好：使用代码引用
"帮我看看 ChatController.java 第 50-60 行的逻辑"
```

#### 精简提示词
```markdown
❌ 不好："请帮我看看这个代码，它有很多问题，我需要你仔细分析..."
✅ 好："检查 ChatController.java:50-60 的异常处理逻辑"
```

#### 明确指定范围
```markdown
❌ 不好："修复所有错误"
✅ 好："修复 ChatController.java 中的空指针异常"
```

### 2. 控制上下文长度

#### 关闭不必要的文件
- 只打开需要编辑的文件
- 关闭不相关的标签页
- 使用 `.cursorignore` 排除大文件

#### 限制文件扫描范围
在 Cursor 设置中：
- 限制代码库扫描深度
- 排除 `node_modules`、`target` 等目录
- 只索引必要的文件类型

### 3. 使用 Cursor 的高效功能

#### 使用 Composer（多文件编辑）
```markdown
❌ 不好：逐个文件询问修改
✅ 好：一次性说明需求，让 Composer 批量处理
```

#### 使用 Chat（单文件快速问答）
- 简单问题用 Chat，避免加载过多上下文
- 复杂重构用 Composer

#### 使用代码补全而非完整生成
- 优先使用 Tab 补全（消耗更少 token）
- 只在必要时使用完整代码生成

### 4. 优化代码结构

#### 使用函数和模块化
```java
// ❌ 不好：重复代码增加 token
public void method1() {
    // 100 行代码
}
public void method2() {
    // 100 行相同代码
}

// ✅ 好：提取公共方法
private void commonLogic() {
    // 100 行代码
}
public void method1() {
    commonLogic();
}
```

#### 使用注释和文档
- 清晰的注释帮助 AI 理解代码
- 减少 AI 需要"猜测"的代码量

### 5. 对话管理技巧

#### 避免重复发送相同内容
```markdown
❌ 不好：每次对话都重新发送完整代码
✅ 好：引用之前的对话："按照刚才的方案继续"
```

#### 使用对话历史
- 在同一对话中继续，而不是开启新对话
- 利用上下文，避免重复说明

#### 批量处理请求
```markdown
❌ 不好：
"修改 A 文件"
"修改 B 文件"  
"修改 C 文件"

✅ 好：
"同时修改 A、B、C 文件，统一添加异常处理"
```

### 6. 配置优化

#### 调整 Cursor 设置
- 限制上下文窗口大小
- 关闭自动代码扫描（如果不需要）
- 使用更轻量的模型（如果支持）

#### 使用 `.cursorignore`
创建 `.cursorignore` 文件：
```
node_modules/
target/
*.log
*.class
dist/
build/
```

### 7. 代码审查策略

#### 分阶段审查
```markdown
❌ 不好：一次性审查整个项目
✅ 好：
1. 先审查核心逻辑
2. 再审查具体实现
3. 最后审查细节
```

#### 使用代码引用
```markdown
❌ 不好：复制粘贴完整代码块
✅ 好：使用文件路径和行号引用
"检查 ChatController.java:45-60"
```

### 8. 实际使用示例

#### 示例 1：代码审查
```markdown
❌ 低效方式：
[粘贴 500 行代码]
"请审查这段代码"

✅ 高效方式：
"审查 ChatController.java 的 POST /api/chat 端点，
重点关注异常处理和参数验证"
```

#### 示例 2：功能添加
```markdown
❌ 低效方式：
"添加用户认证功能"
[AI 生成大量代码]
"不对，我只要 JWT"
[重新生成]

✅ 高效方式：
"在 ChatController.java 中添加 JWT 认证，
参考现有的 AuthService"
```

### 9. 监控和优化

#### 定期检查使用情况
- 查看 Cursor 的使用统计
- 识别高消耗的操作
- 优化重复性任务

#### 建立最佳实践
- 记录高效的对话模式
- 创建常用提示词模板
- 团队共享优化经验

### 10. 高级技巧

#### 使用 MCP 减少重复查询
- 配置 MCP 服务器缓存常用数据
- 避免重复查询相同信息

#### 本地开发优先
- 简单问题先用本地工具解决
- 复杂问题再使用 AI

## 快速参考清单

- 使用代码引用而非完整代码
- 精简提示词，明确需求
- 关闭不必要的文件
- 使用对话历史，避免重复
- 批量处理相关请求
- 配置 `.cursorignore` 排除大文件
- 优先使用代码补全
- 分阶段处理复杂任务

## 预期效果

采用这些方法后，预计可以：
- 减少 30-50% 的 token 使用量
- 提高对话效率
- 降低使用成本

**关键原则：只发送必要的上下文，明确表达需求，充分利用对话历史。**

---

## 📈 Token 使用监控与优化总结

### 理解 Token 交互的关键点

1. **每次交互 = 输入 Token + 输出 Token**
   - 输入包括：你的提示词 + 文件内容 + 对话历史
   - 输出包括：AI 的回复 + 生成的代码

2. **不同场景的 Token 消耗**
   - Tab 补全：50-500 tokens（最省）
   - Chat 对话：500-5000 tokens（中等）
   - 代码生成：500-3000 tokens（中等）
   - Composer 编辑：5000-50000 tokens（最多）

3. **成本估算**
   - 根据你的使用记录：约 $0.16-$0.18/次（50-55 万 tokens）
   - 简单操作：$0.0003-$0.003
   - 复杂操作：$0.03-$0.18

### 优化策略优先级

**高优先级（立即见效）：**
1. ✅ 使用代码引用而非完整代码
2. ✅ 关闭不必要的文件
3. ✅ 使用对话历史，避免重复

**中优先级（显著减少）：**
4. ✅ 精简提示词，明确需求
5. ✅ 批量处理相关请求
6. ✅ 配置 `.cursorignore` 排除大文件

**低优先级（长期优化）：**
7. ✅ 优化代码结构，减少重复
8. ✅ 建立提示词模板
9. ✅ 定期监控和调整

### 实际应用建议

**日常开发：**
- 简单问题用 Chat（500-2000 tokens）
- 复杂重构用 Composer（但先明确需求）
- 代码补全优先用 Tab（50-500 tokens）

**代码审查：**
- 分阶段审查，不要一次性审查整个项目
- 使用代码引用，不要粘贴完整代码
- 明确审查重点，避免 AI 生成过多内容

**功能开发：**
- 先明确需求，再生成代码
- 参考现有代码，减少生成量
- 分步骤实现，避免一次性生成大量代码

---

**记住：理解 Token 交互数量是优化使用的基础，只有知道消耗在哪里，才能有针对性地优化！**